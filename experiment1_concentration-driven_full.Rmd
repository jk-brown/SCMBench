---
title: "Experiment 1 - Concentration Full"
author: "Joe Brown"
date: "2025-05-14"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Goal

The goal for this file is to begin by running Hector + Matilda in concentration-driven mode. This means that we will be constraining history to CO2 concentrations and then using perturbed parameters to make future projections. 

Experiment 1 - running Hector + Matilda prescribing CO2 concentrations for the historic and future time periods. 

# Set-up

## Load libraries and utils 

```{r, message=FALSE}

source("set-up/libraries.R")
source("set-up/utils.R")

```

## Load ini files

```{r}

source("set-up/load_ini.R")

```

## Load perturbed parameter data frame

This step loads a pre-generated perturbed parameter ensemble (PPE) that will be used across all Hector + Matilda experiments. The PPE was generated once using Hector defaults and saved to ensure consistent sampling across model configurations. Parameters are automatically "chunked" for parallel processing.

```{r}

source("set-up/load_params.R")

```

# Experiment 1 Running the model: Constrained to C02 in history & future

The goal here is to run Hector + Matilda in "concentration-driven" mode by constraining historical and future CO2 concentrations.

## Load CO2 data for constraint

```{r}
# scenario names
ssp_scenarios <- c("ssp119", "ssp126", "ssp245", "ssp370", "ssp585")
# directory where constraint data are saved
constraint_dir <- here("data", "processed")

# Load scenario specific co2 constraint data
constraint_data_list <- lapply(ssp_scenarios, function(ssp) {
  
  read.csv(file.path(constraint_dir, paste0(ssp, "_CO2_constraint-full.csv")))
  
})
# edit list names
names(constraint_data_list) <- names(ini_list)
```

## Run model 

Run Hector over entire PPE for each SSP scenario in the scenario list

```{r}
start_time <- Sys.time()
matilda_result_exp1 <- matilda_conc_driven(param_chunks, ini_list, constraint_data_list)
end_time <- Sys.time()

elapsed_time <- end_time - start_time
print(elapsed_time)

```

Correct the run_numbers for each of element in the result list

```{r}

matilda_result_exp1 <- fix_run_numbers(matilda_result_exp1)

```

## Check for and deal with NAs

Get the runs that produced NAs:
```{r}

lapply(matilda_result_exp1, function(df) {
  
  get_na_runs(df, verbose = T)
  
})

```
Remove NAs if desired:
```{r}

matilda_result_exp1 <- lapply(matilda_result_exp1, function(df) {
  
  remove_na_runs(df, verbose = T)
  
})

```

# Save model runs 

```{r}
# define directory path
result_dir <- here("output", "model_results")

# create directory if one does not exist
if (!dir.exists(result_dir)) {
  dir.create(result_dir, recursive = T)
}

# identify the file path
file_name <- "exp1_matilda_results_concentration_full_NA-rm.RDS"
file_path <- file.path(result_dir, file_name)

# save 
saveRDS(matilda_result_exp1, file_path)
```


# Normalize GMST and GSAT 

Normalize temperature data (GMST) to the 1850-1900 reference period to use for weighting the ensemble members.

```{r}
normalize_exp1_result <- lapply(matilda_result_exp1, function(df) {
  
  normalize_to_reference(df, 1850, 1900, variables = "gmst")
  
})

```


# Weight ensemble

Weight the ensembles from experiment one using the GMST and CO2 scoring criterion.

Load the scoring criterion:
```{r}

source("set-up/load_scoring_criterion.R")

```

Compute weights:
```{r}

weights_exp1 <- lapply(matilda_result_exp1, function(df) {
  
  weight_ensemble(df, gmst_criterion)
  
})

```

Identify missing rows due to NAs in model output:
```{r}

missing_runs <- lapply(weights_exp1, function(df) {
  
  setdiff(1:n, df$run_number)
  
})

# print missing runs 
for (scenario in names(missing_runs)) {
  cat("missing runs for", scenario, ":\n")
  print(missing_runs[[scenario]])
  cat("\n")
}

```

## Plotting ensembles with weighting

Merge weights with the model results using `run_number`.
```{r}

weighted_result_exp1 <- Map(function(weights, results) {
  
  merge(results, weights, by = "run_number")
  
}, weights_exp1, matilda_result_exp1)

weighted_result_exp1_df <- do.call(rbind, weighted_result_exp1)

```

Plot gmst ensemble with weights
```{r}
ggplot(data = subset(weighted_result_exp1_df, variable == "gmst" & year < 2101)) +
  geom_line(aes(x = year, y = value, group = run_number, color = mc_weight, alpha = mc_weight)) +
  scale_color_gradient(low = "dodgerblue", high = "dodgerblue4")+
  scale_alpha_continuous(range = c(0, 1)) +
  geom_line(data = hist_temp_norm, aes(x = year, y = value), color = "red")+
  facet_wrap(~scenario) +
  theme_light()

```

# Compute median warming for each 

Here we want to compute the probability distribution of warming.

First normalize values for `global_tas` to the 2005-2014 reference period:

```{r}
normalize_exp1_result_gsat <- lapply(matilda_result_exp1, function(df) {
  
  normalize_to_reference(df, 2005, 2014, variables = "global_tas")
  
})
```

Create new metric object:
maybe Add this to experimental set-up script so I can use it for all the experiments. 
```{r}
# creating median end-of-century warming (GSAT)
eoc_warming <- new_metric(GLOBAL_TAS(), 2081:2100, median)

```

Use the eoc_warming metric to compute median eoc warming for each run:
```{r}
# computing median end-of-century warming
eoc_warming_values_exp1 <- lapply(names(normalize_exp1_result_gsat), function(scenario_name) {
  
  df <- normalize_exp1_result_gsat[[scenario_name]]
   
  metric_values <- metric_calc(df, eoc_warming)
  
  metric_values$scenario <- scenario_name
  
  return(metric_values)
   
 })

# merge weights with new metrics 
weighted_metrics_exp1 <- Map(function(weights, metric_results) {
  
  merge(metric_results, weights, by = "run_number")
  
}, weights_exp1, eoc_warming_values_exp1)

```

Make a weighted metrics dataframe for plotting:
```{r}

weighted_metrics_exp1_df <- do.call(rbind, weighted_metrics_exp1)

```

Save the weighted metrics data frame:

```{r}
# define directory path
result_dir <- here("output", "warming_results")

# create directory if one does not exist
if (!dir.exists(result_dir)) {
  dir.create(result_dir, recursive = T)
}

# identify the file path
file_name <- "exp1_weighted_gsat_metric.csv"
file_path <- file.path(result_dir, file_name)

# save 
write.csv(weighted_metrics_exp1_df, file_path)

```

Preliminary visualization of warming PDF. Here we want to plot a weighted PDF curve with a facet for scenario:
```{r}

ggplot(weighted_metrics_exp1_df, aes(x = metric_result)) +
  stat_density(aes(weight = mc_weight), geom = "line", color = "dodgerblue", linewidth = 1.2, adjust = 1) +
  labs(
    title = "Likelihood-weighted Warming Distribution",
    x = "End-of-century Warming (Â°C)",
    y = "Density (weighted)"
  ) +
  theme_light() +
  facet_wrap(~scenario)

```
OKAY that works -- and I will want to overlay the other experiments to show how the warming distribution is affected by the historical configuration. 

## Computing probabilities for end of century warming

Here I want to compute probabilities for a range of different temperatures.

```{r}
# define bins that represent the temperature ranges of interest
temp_ranges <- c(0, 1, 1.5, 2, 2.5, 3, 3.5, 4, 4.5, Inf)

# computing probabilities
temp_prob_exp1 <- lapply(names(weighted_metrics_exp1), function(df_name) {
  
  # copy data 
  df <- weighted_metrics_exp1[[df_name]]
  
  # run probability calculator
  prob_result <- prob_calc(df$metric_result, 
                           bins = temp_ranges, 
                           scores = df$mc_weight)
  
  # add scenario column to the result
  prob_result$scenario <- df_name
  
  return(prob_result)
})

# give element names 
names(temp_prob_exp1) <- names(weights_exp1)

```

Save probability data:

```{r}
# define directory path
result_dir <- here("output", "probability_results")

# create directory if one does not exist
if (!dir.exists(result_dir)) {
  dir.create(result_dir, recursive = T)
}

# identify the file path
file_name <- "exp1_weighted_gsat_probability.rds"
file_path <- file.path(result_dir, file_name)

# save 
saveRDS(temp_prob_exp1, file_path)
```

Create a probability df and save:
```{r}
exp1_probability_df <- do.call(rbind, temp_prob_exp1)

# define directory path
result_dir <- here("output", "probability_results")

# create directory if one does not exist
if (!dir.exists(result_dir)) {
  dir.create(result_dir, recursive = T)
}

# identify the file path
file_name <- "exp1_weighted_gsat_probability.csv"
file_path <- file.path(result_dir, file_name)

# save 
write.csv(exp1_probability_df, file_path, row.names = F)
```


# Computing metrics for median warming in near term (2021-2040)

Create new metric object for near term period:
maybe Add this to experimental set-up script so I can use it for all the experiments. 
```{r}
# creating median end-of-century warming (GSAT)
nt_warming <- new_metric(GLOBAL_TAS(), 2021:2040, median)

```

Use the nt_warming metric to compute median near term warming for each run:
```{r}
# computing median end-of-century warming
nt_warming_values_exp1 <- lapply(names(normalize_exp1_result_gsat), function(scenario_name) {
  
  df <- normalize_exp1_result_gsat[[scenario_name]]
   
  metric_values <- metric_calc(df, nt_warming)
  
  metric_values$scenario <- scenario_name
  
  return(metric_values)
   
 })

# merge weights with new metrics 
weighted_nt_metrics_exp1 <- Map(function(weights, metric_results) {
  
  merge(metric_results, weights, by = "run_number")
  
}, weights_exp1, nt_warming_values_exp1)

```

Make a weighted metrics dataframe for plotting:
```{r}

weighted_nt_metrics_exp1_df <- do.call(rbind, weighted_nt_metrics_exp1)

```

Save the weighted metrics data frame:

```{r}
# define directory path
result_dir <- here("output", "warming_results")

# create directory if one does not exist
if (!dir.exists(result_dir)) {
  dir.create(result_dir, recursive = T)
}

# identify the file path
file_name <- "exp1_weighted_gsat_metric_near-term.csv"
file_path <- file.path(result_dir, file_name)

# save 
write.csv(weighted_nt_metrics_exp1_df, file_path)

```

Preliminary visualization of warming PDF. Here we want to plot a weighted PDF curve with a facet for scenario:
```{r}

ggplot(weighted_nt_metrics_exp1_df, aes(x = metric_result)) +
  stat_density(aes(weight = mc_weight), geom = "line", color = "dodgerblue", linewidth = 1.2, adjust = 1) +
  labs(
    title = "Likelihood-weighted Warming Distribution",
    x = "2021-2040 Median Warming (Â°C)",
    y = "Density (weighted)"
  ) +
  theme_light() +
  facet_wrap(~scenario)

```
## Computing probabilities for near term warming (2021-2040)

Here I want to compute probabilities for a range of different temperatures.

```{r}
# computing probabilities
temp_prob_nt_exp1 <- lapply(names(weighted_nt_metrics_exp1), function(df_name) {
  
  # copy data 
  df <- weighted_nt_metrics_exp1[[df_name]]
  
  # run probability calculator
  prob_result <- prob_calc(df$metric_result, 
                           bins = temp_ranges, 
                           scores = df$mc_weight)
  
  # add scenario column to the result
  prob_result$scenario <- df_name
  
  return(prob_result)
})

# give element names 
names(temp_prob_nt_exp1) <- names(weights_exp1)

```

Save probability data:

```{r}
# define directory path
result_dir <- here("output", "probability_results")

# create directory if one does not exist
if (!dir.exists(result_dir)) {
  dir.create(result_dir, recursive = T)
}

# identify the file path
file_name <- "exp1_weighted_gsat_probability_near-term.rds"
file_path <- file.path(result_dir, file_name)

# save 
saveRDS(temp_prob_nt_exp1, file_path)
```

Create a probability df and save:
```{r}
exp1_nt_probability_df <- do.call(rbind, temp_prob_nt_exp1)

# define directory path
result_dir <- here("output", "probability_results")

# create directory if one does not exist
if (!dir.exists(result_dir)) {
  dir.create(result_dir, recursive = T)
}

# identify the file path
file_name <- "exp1_weighted_gsat_probability_near-term.csv"
file_path <- file.path(result_dir, file_name)

# save 
write.csv(exp1_nt_probability_df, file_path, row.names = F)
```



# Computing metrics for median warming in mid term (2040-2060)

Create new metric object for mid-term period:

maybe Add this to experimental set-up script so I can use it for all the experiments. 
```{r}
# creating median end-of-century warming (GSAT)
mt_warming <- new_metric(GLOBAL_TAS(), 2040:2060, median)

```

Use the nt_warming metric to compute median near term warming for each run:
```{r}
# computing median mid-term warming
mt_warming_values_exp1 <- lapply(names(normalize_exp1_result_gsat), function(scenario_name) {
  
  df <- normalize_exp1_result_gsat[[scenario_name]]
   
  metric_values <- metric_calc(df, mt_warming)
  
  metric_values$scenario <- scenario_name
  
  return(metric_values)
   
 })

# merge weights with new metrics 
weighted_mt_metrics_exp1 <- Map(function(weights, metric_results) {
  
  merge(metric_results, weights, by = "run_number")
  
}, weights_exp1, mt_warming_values_exp1)

```

Make a weighted metrics dataframe for plotting:
```{r}

weighted_mt_metrics_exp1_df <- do.call(rbind, weighted_mt_metrics_exp1)

```

Save the weighted metrics data frame:

```{r}
# define directory path
result_dir <- here("output", "warming_results")

# create directory if one does not exist
if (!dir.exists(result_dir)) {
  dir.create(result_dir, recursive = T)
}

# identify the file path
file_name <- "exp1_weighted_gsat_metric_mid-term.csv"
file_path <- file.path(result_dir, file_name)

# save 
write.csv(weighted_mt_metrics_exp1_df, file_path)

```

Preliminary visualization of warming PDF. Here we want to plot a weighted PDF curve with a facet for scenario:
```{r}

ggplot(weighted_mt_metrics_exp1_df, aes(x = metric_result)) +
  stat_density(aes(weight = mc_weight), geom = "line", color = "dodgerblue", linewidth = 1.2, adjust = 1) +
  labs(
    title = "Likelihood-weighted Warming Distribution",
    x = "2041-2060 Median Warming (Â°C)",
    y = "Density (weighted)"
  ) +
  theme_light() +
  facet_wrap(~scenario)

```

## Computing probabilities for mid-term warming (2041-2060)

Here I want to compute probabilities for a range of different temperatures.

```{r}
# computing probabilities
temp_prob_mt_exp1 <- lapply(names(weighted_mt_metrics_exp1), function(df_name) {
  
  # copy data 
  df <- weighted_mt_metrics_exp1[[df_name]]
  
  # run probability calculator
  prob_result <- prob_calc(df$metric_result, 
                           bins = temp_ranges, 
                           scores = df$mc_weight)
  
  # add scenario column to the result
  prob_result$scenario <- df_name
  
  return(prob_result)
})

# give element names 
names(temp_prob_mt_exp1) <- names(weights_exp1)

```

Save probability data:

```{r}
# define directory path
result_dir <- here("output", "probability_results")

# create directory if one does not exist
if (!dir.exists(result_dir)) {
  dir.create(result_dir, recursive = T)
}

# identify the file path
file_name <- "exp1_weighted_gsat_probability_mid-term.rds"
file_path <- file.path(result_dir, file_name)

# save 
saveRDS(temp_prob_mt_exp1, file_path)
```

Create a probability df and save:
```{r}
exp1_mt_probability_df <- do.call(rbind, temp_prob_mt_exp1)

# define directory path
result_dir <- here("output", "probability_results")

# create directory if one does not exist
if (!dir.exists(result_dir)) {
  dir.create(result_dir, recursive = T)
}

# identify the file path
file_name <- "exp1_weighted_gsat_probability_mid-term.csv"
file_path <- file.path(result_dir, file_name)

# save 
write.csv(exp1_mt_probability_df, file_path, row.names = F)
```
