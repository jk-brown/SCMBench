---
title: "Experiment 2 - Concentration History"
author: "Joe Brown"
date: "2025-05-27"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Goal

The goal for this file is to begin by running Hector + Matilda in concentration-driven mode in history and then switching to emissions-driven future. This means that we will be constraining history to CO2 concentrations and then using emissions with the carbon cycle and perturbed parameters to make future projections.

# Set-up

## Load libraries and utils 

```{r, message=FALSE}

source("set-up/libraries.R")
source("set-up/utils.R")

```

## Load ini files

```{r}

source("set-up/load_ini.R")

```

## Load perturbed parameter data frame

This step loads a pre-generated perturbed parameter ensemble (PPE) that will be used across all Hector + Matilda experiments. The PPE was generated once using Hector defaults and saved to ensure consistent sampling across model configurations. Parameters are automatically "chunked" for parallel processing.

```{r}

source("set-up/load_params.R")

```


Experiment 2 Running the model: Constrained to C02 in history with emissions driven future

The goal here is to run Hector + Matilda in "concentration-driven" mode by constraining historical CO2 concentrations.

## Load CO2 data for constraint

```{r}
# scenario names
ssp_scenarios <- c("ssp119", "ssp126", "ssp245", "ssp370", "ssp585")
# directory where constraint data are saved
constraint_dir <- here("data", "processed")

# Load scenario specific co2 constraint data
constraint_data_list <- lapply(ssp_scenarios, function(ssp) {
  
  read.csv(file.path(constraint_dir, paste0(ssp, "_CO2_constraint-historical.csv")))
  
})

# edit list names
names(constraint_data_list) <- names(ini_list)
```
## Run model 

Run Hector over entire PPE for each SSP scenario in the scenario list

```{r}
start_time <- Sys.time()
matilda_result_exp2 <- matilda_conc_driven(param_chunks, ini_list, constraint_data_list)
end_time <- Sys.time()

elapsed_time <- end_time - start_time
print(elapsed_time)

```

Correct the run_numbers for each of element in the result list

```{r}

matilda_result_exp2 <- fix_run_numbers(matilda_result_exp2)

```

## Check for and deal with NAs

Get the runs that produced NAs:
```{r}

lapply(matilda_result_exp2, function(df) {
  
  get_na_runs(df, verbose = T)
  
})

```

Remove NAs if desired:
```{r}

matilda_result_exp2 <- lapply(matilda_result_exp2, function(df) {
  
  remove_na_runs(df, verbose = T)
  
})

```

# Save model runs 

```{r}
# define directory path
result_dir <- here("output", "model_results")

# create directory if one does not exist
if (!dir.exists(result_dir)) {
  dir.create(result_dir, recursive = T)
}

# identify the file path
file_name <- "exp2_matilda_results_concentration_historic_NA-rm.RDS"
file_path <- file.path(result_dir, file_name)

# save 
saveRDS(matilda_result_exp2, file_path)
```


# Normalize GMST and GSAT 

Normalize temperature data (GMST) to the 1850-1900 reference period to use for weighting the ensemble members.

```{r}
normalize_exp2_result <- lapply(matilda_result_exp2, function(df) {
  
  normalize_to_reference(df, 1850, 1900, variables = "gmst")
  
})

```


# Weight ensemble

Weight the ensembles from experiment one using the GMST and CO2 scoring criterion.

Load the scoring criterion:
```{r}

source("set-up/load_scoring_criterion.R")

```

Compute weights:
```{r}

weights_exp2 <- lapply(matilda_result_exp2, function(df) {
  
  weight_ensemble(df, gmst_criterion, co2_criterion)
  
})

```

Identify missing rows due to NAs in model output:
```{r}

missing_runs <- lapply(weights_exp2, function(df) {
  
  setdiff(1:n, df$run_number)
  
})

# print missing runs 
for (scenario in names(missing_runs)) {
  cat("missing runs for", scenario, ":\n")
  print(missing_runs[[scenario]])
  cat("\n")
}

```

## Plotting ensembles with weighting

Merge weights with the model results using `run_number`.
```{r}

weighted_result_exp2 <- Map(function(weights, results) {
  
  merge(results, weights, by = "run_number")
  
}, weights_exp2, matilda_result_exp2)

weighted_result_exp2_df <- do.call(rbind, weighted_result_exp2)

```

Plot gmst ensemble with weights:
```{r}
ggplot(data = subset(weighted_result_exp2_df, variable == "gmst" & year < 2101)) +
  geom_line(aes(x = year, y = value, group = run_number, color = mc_weight, alpha = mc_weight)) +
  scale_color_gradient(low = "dodgerblue", high = "dodgerblue4")+
  scale_alpha_continuous(range = c(0, 1)) +
  geom_line(data = hist_temp_norm, aes(x = year, y = value), color = "red")+
  facet_wrap(~scenario) +
  theme_light()

```

Just a quick visualization to see if everything is looking good so far. This always makes for a decent preliminary figure to show whether the weighting process is being effective.

# Compute median warming for each 

Here we want to compute the probability distribution of warming.

First normalize values for `global_tas` to the 2005-2014 reference period:

```{r}
normalize_exp2_result_gsat <- lapply(matilda_result_exp2, function(df) {
  
  normalize_to_reference(df, 2005, 2014, variables = "global_tas")
  
})
```

Create new metric object:
maybe Add this to experimental set-up script so I can use it for all the experiments. 
```{r}
# creating median end-of-century warming (GSAT)
eoc_warming <- new_metric(GLOBAL_TAS(), 2081:2100, median)

```

Use the eoc_warming metric to compute median eoc warming for each run:
```{r}
# computing median end-of-century warming
eoc_warming_values_exp2 <- lapply(names(normalize_exp2_result_gsat), function(scenario_name) {
  
  df <- normalize_exp2_result_gsat[[scenario_name]]
   
  metric_values <- metric_calc(df, eoc_warming)
  
  metric_values$scenario <- scenario_name
  
  return(metric_values)
   
 })

# merge weights with new metrics 
weighted_metrics_exp2 <- Map(function(weights, metric_results) {
  
  merge(metric_results, weights, by = "run_number")
  
}, weights_exp2, eoc_warming_values_exp2)

```

Make a weighted metrics dataframe for plotting:
```{r}

weighted_metrics_exp2_df <- do.call(rbind, weighted_metrics_exp2)

```

Save the weighted metrics data frame:

```{r}
# define directory path
result_dir <- here("output", "warming_results")

# create directory if one does not exist
if (!dir.exists(result_dir)) {
  dir.create(result_dir, recursive = T)
}

# identify the file path
file_name <- "exp2_weighted_gsat_metric.csv"
file_path <- file.path(result_dir, file_name)

# save 
write.csv(weighted_metrics_exp2_df, file_path)

```

Preliminary visualization of warming PDF. Here we want to plot a weighted PDF curve with a facet for scenario:
```{r}

ggplot(weighted_metrics_exp2_df, aes(x = metric_result)) +
  stat_density(aes(weight = mc_weight), geom = "line", color = "dodgerblue", size = 1.2, adjust = 1) +
  labs(
    title = "Likelihood-weighted Warming Distribution",
    x = "End-of-century Warming (Â°C)",
    y = "Density (weighted)"
  ) +
  theme_light() +
  facet_wrap(~scenario)

```
OKAY that works -- and I will want to overlay the other experiments to show how the warming distribution is affected by the historical configuration. 


