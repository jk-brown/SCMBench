---
title: 'Testing Hypothesis #1'
author: "Joe Brown"
date: "2025-06-17"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# H1 background

### Mechanism

In the fully emissions-driven (ED) configuration, carbon cycle parameter uncertainty affects both historical and future atmospheric CO2. This allows historical carbon cycle variability to propagate forward and influence future radiative forcing and warming. When CO2 concentrations are prescribed, as in the concentration-driven (CD) and concentration-emission (CE) configurations, the carbon cycle uncertainty is suppressed from impacting overal model response uncertainty. We want to know whether this results in a significant difference in the spread of the response uncertainty. And if it does, can we quantify by how much?

### Prediction

The ED configuration will exhibit a wider distribution of projected late century (2081-2100) global temperature anomaly, followed by the CE configuration, and then the concentration-driven configuration (CD), because the ED configuration allows the uncertainty from the historical carbon cycle to propagate to future Earth system respopnse. 

i.e.: Uncertainty: CD < CE < ED

# Set-up

### Load libraries and utils 

```{r, message=FALSE}

source("set-up/libraries.R")
source("set-up/utils.R")

```

# Data 

Need Matilda results from each of the three configurations we are testing.

We will use this to compute the `median gsat for 2081-2100` relative to the 1995-2014 reference period:

Load experiment results:
```{r}

matilda_result_exp1 <- readRDS("output/model_results/exp1_matilda_results_concentration_full_NA-rm.RDS")
matilda_result_exp2 <- readRDS("output/model_results/exp2_matilda_results_concentration_historic_NA-rm.RDS")
matilda_result_exp3 <- readRDS("output/model_results/exp3_matilda_results_concentration_historic_NA-rm.RDS")

```

For each experiment, compute `global_tas` relative to 1995-2014 reference period:
```{r}

# For exp1
matilda_result_exp1_norm_gsat <- lapply(matilda_result_exp1, function(df) {
  normalize_to_reference(df, 1995, 2014, variables = "global_tas")
}) 

# For exp2
matilda_result_exp2_norm_gsat <- lapply(matilda_result_exp2, function(df) {
  normalize_to_reference(df, 1995, 2014, variables = "global_tas")
}) 

# For exp3
matilda_result_exp3_norm_gsat <- lapply(matilda_result_exp3, function(df) {
  normalize_to_reference(df, 1995, 2014, variables = "global_tas")
}) 


```

# Compute late century temperature change

Define the metric of interest:
```{r}

# creating median late century warming metric using global_tas
lc_warming <- new_metric(GLOBAL_TAS(), 2081:2100, median)

```

Compute median late century temperature chmage for each experiment:
```{r}
# For exp1
lc_gsat_values_exp1 <- lapply(names(matilda_result_exp1_norm_gsat), function(scenario_name) {
  
  df <- matilda_result_exp1_norm_gsat[[scenario_name]]
   
  metric_values <- metric_calc(df, lc_warming)
  
  metric_values$scenario <- scenario_name
  
  metric_values$config <- "CD"
  
  return(metric_values)
   
 })

lc_gsat_values_exp1_df <- do.call(rbind, lc_gsat_values_exp1)

# For exp2
lc_gsat_values_exp2 <- lapply(names(matilda_result_exp2_norm_gsat), function(scenario_name) {
  
  df <- matilda_result_exp2_norm_gsat[[scenario_name]]
   
  metric_values <- metric_calc(df, lc_warming)
  
  metric_values$scenario <- scenario_name
  
  metric_values$config <- "CE"
  
  return(metric_values)
   
 })

lc_gsat_values_exp2_df <- do.call(rbind, lc_gsat_values_exp2)

# For exp3
lc_gsat_values_exp3 <- lapply(names(matilda_result_exp3_norm_gsat), function(scenario_name) {
  
  df <- matilda_result_exp3_norm_gsat[[scenario_name]]
   
  metric_values <- metric_calc(df, lc_warming)
  
  metric_values$scenario <- scenario_name
  
  metric_values$config <- "ED"
  
  return(metric_values)
   
 })

lc_gsat_values_exp3_df <- do.call(rbind, lc_gsat_values_exp3)
```

Combine to a single data frame:
```{r}

lc_gsat_all <- rbind(lc_gsat_values_exp1_df, lc_gsat_values_exp2_df, lc_gsat_values_exp3_df)

```

# Summarize data to qunatify uncertainty spread

Summarize data for each exp-scenario group and calculate quantiles (5-95), median, and uncertainty spread:
```{r}

lc_gsat_summary <- lc_gsat_all %>% 
  group_by(config, scenario) %>% 
  summarize(
    median = quantile(metric_result, probs = 0.50), 
    ci_5 = quantile(metric_result, probs = 0.05), 
    ci_95 = quantile(metric_result, probs = 0.95), 
    spread = ci_95 - ci_5, 
    .groups = "drop")

head(lc_gsat_summary)

```

Now we want to bootstrap the uncertainty spread for each config-scenario combination so we can compare the differences in uncertainty spread:
```{r}
# bootstrap parameters
n_boot <- 10000
n_sample <- 1000

# bootstrap function
bootstrap_spread <- function(values, n_boot, n_sample) {
  
  replicate(n_boot, {
    resample = sample(values, size = n_sample, replace = TRUE)
    ci_5 = quantile(resample, probs = 0.05) 
    ci_95 = quantile(resample, probs = 0.95) 
    ci_95 - ci_5
  })
}

# split data into config-scenario groups
lc_gsat_split <- split(lc_gsat_all, list(lc_gsat_all$config, lc_gsat_all$scenario))

# apply bootstrap
bootstrap_lc_gsat_spread <- lapply(names(lc_gsat_split), function(group_name) {
  
  group_data <- lc_gsat_split[[group_name]]
  spread_vals <- bootstrap_spread(group_data$metric_result, n_boot, n_sample)
  
  # get config and scenario names from the "group_name"
  parts <- unlist(strsplit(group_name, split = "\\."))
  config <- parts[1]
  scenario <- parts[2]
  
  data.frame(
    config = config,
    scenario = scenario,
    spread = spread_vals
  )
})

names(bootstrap_lc_gsat_spread) <- names(lc_gsat_split)

bootstrap_lc_gsat_spread_df <- do.call(rbind, bootstrap_lc_gsat_spread)
```

The result of the above is a new data frame with 10,000 spread estimates for each config-SSP combination. This will allow us to compute summary statistics on the uncertainty spread of our data and also allow us to make comparisons about uncertainty spread across config types.

We can use the bootstrap data to produce statistical summaries fro the uncertainty spread of the different configs across SSP scenarios:
```{r}

spread_summary_df <- bootstrap_lc_gsat_spread_df %>% 
  group_by(config, scenario) %>% 
  summarize(
    mean_spread = mean(spread),
    ci_5 = quantile(spread, 0.05), 
    ci_95 = quantile(spread, 0.95), 
    .groups = "drop"
  )

```

Before moving on to the next step of looking at pairwise spread differences, we can plot the mean + CI of the spread by config type for each SSP:

```{r}
ggplot(data = spread_summary_df) +
  geom_bar(aes(x = config, y = mean_spread, fill = config), stat = "identity") +
  geom_errorbar(aes(x = config, ymin = ci_5, ymax = ci_95), width = 0.25) +
  facet_wrap(~scenario) +
  theme_light()
```
The barplot reveals a consistent difference in the uncertainty spread across model configurations. In nearly every SSP scenario, the concentration-driven (CD) configuration exhibits a significantly narrower spread in late century global temperature projections compared to the emissions-driven (ED) and emissions-with-carbon-cycle (EC) configurations. This suggests that constraining historical CO2 concentrations suppresses carbon cycle variability, which in turn limits the propagation of uncertainty into future projections. This reduced spread could lead to underestimating the likelihood of exceeding key temperature thresholds. This pattern holds across all scenarios tested here, with the exception of SSP5-8.5, where the difference between configurations appears less pronounced.

This analysis also provides useful perspective on a broader modeling practice: Earth system models (ESMs) are typically run in concentration-driven mode for both historical and scenario simulations, meaning CO2 concentrations are prescribed rather than simulated. The SCM experiment here isolates the consequences of that choice. It shows that prescribing CO₂ concentrations—while useful for consistency—can obscure the uncertainty introduced by carbon cycle processes, especially when estimating the likelihood of warming outcomes. This observation reinforces arguments made by Sanderson et al. (2024), who caution that concentration-driven frameworks may under-represent structural uncertainty in future climate projections.

### Pairwise comparisons

Now we want to know the pariwise differences so we can make quantitative statements about the spread differences between the configuration types:
```{r}
# List of SSPs and config pairs
ssp_list <- unique(lc_gsat_all$scenario)
config_pairs <- list(
  c("CD", "CE"),
  c("CD", "ED"),
  c("CE", "ED")
)

# Initialize results list
pairwise_results <- list()

# Loop through SSPs and config pairs
for (ssp in ssp_list) {
  for (pair in config_pairs) {
    
    cfg1 <- pair[1]
    cfg2 <- pair[2]
    
    # Build the key names to match the list
    name1 <- paste(cfg1, ssp, sep = ".")
    name2 <- paste(cfg2, ssp, sep = ".")
    
    # Make sure both are in your list
    if (!(name1 %in% names(bootstrap_lc_gsat_spread)) || 
        !(name2 %in% names(bootstrap_lc_gsat_spread))) {
      warning(paste("Missing:", name1, "or", name2))
      next
    }
    
    # Pull spread vectors
    spread1 <- bootstrap_lc_gsat_spread[[name1]]$spread
    spread2 <- bootstrap_lc_gsat_spread[[name2]]$spread
    
    # Subtract
    diff <- spread1 - spread2
    
    # Summarize
    pairwise_results[[paste(ssp, cfg1, cfg2, sep = "_")]] <- data.frame(
      scenario = ssp,
      config_1 = cfg1,
      config_2 = cfg2,
      mean_diff = mean(diff),
      ci_low = quantile(diff, 0.025),
      ci_high = quantile(diff, 0.975)
    )
  }
}

# Combine into final results data frame
pairwise_results_df <- do.call(rbind, pairwise_results)
```

Add a flag to tell which of these is significant:
```{r}

pairwise_results_df$significant <- with(pairwise_results_df, ci_low > 0 | ci_high < 0)
pairwise_results_df$label <- ifelse(pairwise_results_df$significant, "*", "")

```

```{r}
# Define consistent dodge object to reuse across geoms
dodge <- position_dodge(width = 0.6)

ggplot(pairwise_results_df, aes(x = scenario, y = mean_diff, 
                                color = interaction(config_1, config_2),
                                group = interaction(config_1, config_2))) +
  geom_point(position = position_dodge(width = 0.6), size = 3) +
  geom_errorbar(aes(ymin = ci_low, ymax = ci_high),
                position = position_dodge(width = 0.6),
                width = 0.25,
                linewidth = 0.8) +
geom_text(aes(label = label, y = ci_high + 0.02),
            position = dodge,
            vjust = 0, size = 6, 
          show.legend = FALSE) +
  geom_hline(yintercept = 0, linetype = "dashed", color = "gray50") +
  scale_color_brewer(palette = "Set2") +
  labs(
    x = "Scenario",
    y = "Difference in spread (°C)",
    color = "Comparison",
    title = "Spread differences between configuration pairs (90% CI)"
  ) +
  theme_light(base_size = 12) +
  theme(
    panel.grid.minor = element_blank(),
    axis.text.x = element_text(angle = 30, hjust = 1)
  )

```
important interpretation note -- in this type of figure, if the CI line crosses zero, there is a potential that the two configurations do not have a mean uncertainty spread that is different. 

Stab at written results (quantitative values need corrected):
Across all SSP scenarios, the concentration-driven (CD) configuration consistently produced narrower uncertainty spreads than both the emissions-driven (ED) and the concentration-emission (CE) configurations. In every scenario, the mean spread in CD was significantly smaller than in CE and ED. For example, in SSP2-4.5, the spread in CD was on average 0.28 degC narrower than in CE (90% CI: 0.19–0.37 degC), and 0.24 degC narrower than in ED (90% CI: 0.15–0.34 degC). These results support the hypothesis that suppressing carbon cycle uncertainty in the CD configuration artificially (? not sure this is a good use of the word) constrains projected temperature outcomes.

In contrast, differences between CE and ED were small and not statistically significant in any scenario (90% CIs all include zero). This suggests that the act of prescribing CO₂ concentrations, rather than the inclusion or exclusion of the carbon cycle alone, is the primary factor driving the observed reduction in uncertainty.
