---
title: 'Testing Hypothesis #1'
author: "Joe Brown"
date: "2025-06-17"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# H1 background

### Mechanism

In the fully emissions-driven (ED) configuration, carbon cycle parameter uncertainty affects both historical and future atmospheric CO2. This allows historical carbon cycle variability to propagate forward and influence future radiative forcing and warming. When CO2 concentrations are prescribed, as in the concentration-driven (CD) and concentration-emission (CE) configurations, the carbon cycle uncertainty is suppressed from impacting overal model response uncertainty. We want to know whether this results in a significant difference in the spread of the response uncertainty. And if it does, can we quantify by how much?

### Prediction

The ED configuration will exhibit a wider distribution of projected late century (2081-2100) global temperature anomaly, followed by the CE configuration, and then the concentration-driven configuration (CD), because the ED configuration allows the uncertainty from the historical carbon cycle to propagate to future Earth system response. 

i.e.: Uncertainty: CD < CE < ED

# Set-up

### Load libraries and utils 

```{r, message=FALSE}

source("set-up/libraries.R")
source("set-up/utils.R")

```

# Data 

Need Matilda warming projection results from each of the three configurations we are testing.

Load experiment GSAT metric results:
```{r}
lc_gsat_values_exp1_df <- read.csv("output/warming_results/exp1_gsat_metric.csv")
lc_gsat_values_exp1_df$config <- "CD"

lc_gsat_values_exp2_df <- read.csv("output/warming_results/exp2_gsat_metric.csv")
lc_gsat_values_exp2_df$config <- "CE"

lc_gsat_values_exp3_df <- read.csv("output/warming_results/exp3_gsat_metric.csv")
lc_gsat_values_exp3_df$config <- "ED"
```


Combine to a single data frame:
```{r}

lc_gsat_all <- rbind(lc_gsat_values_exp1_df, lc_gsat_values_exp2_df, lc_gsat_values_exp3_df)

```

# Summarize data to qunatify uncertainty spread

Summarize data for each exp-scenario group and calculate quantiles (5-95), median, and uncertainty spread:
```{r}

lc_gsat_summary <- lc_gsat_all %>% 
  group_by(config, scenario) %>% 
  summarize(
    median = quantile(metric_result, probs = 0.50), 
    ci_5 = quantile(metric_result, probs = 0.05), 
    ci_95 = quantile(metric_result, probs = 0.95), 
    spread = ci_95 - ci_5, 
    .groups = "drop")

head(lc_gsat_summary)

```

Now we want to bootstrap the uncertainty spread for each config-scenario combination so we can compare the differences in uncertainty spread:
MOVE THIS TO UTILS SCRIPT
```{r}
set.seed(444)

# bootstrap parameters
n_boot <- 10000
n_sample <- 1000

# bootstrap function
bootstrap_spread <- function(values, n_boot, n_sample) {
  
  # repeat these steps 'n_boot' times
  replicate(n_boot, {
    
    # randomly sample 'n_sample' values with replacement from the input vector 'values'
    resample = sample(values, size = n_sample, replace = TRUE)
    
    # compute stats for the sample
    ci_5 = quantile(resample, probs = 0.05) 
    ci_95 = quantile(resample, probs = 0.95) 
    ci_95 - ci_5
  
    })
}

# split data into config-scenario groups
lc_gsat_split <- split(lc_gsat_all, list(lc_gsat_all$config, lc_gsat_all$scenario))

# apply bootstrap
bootstrap_lc_gsat_spread <- lapply(names(lc_gsat_split), function(group_name) {
  
  # Get the data for this specific configuration-scenario group
  group_data <- lc_gsat_split[[group_name]]
  
  # Apply the bootstrap function to compute 10,000 spread values
  spread_vals <- bootstrap_spread(group_data$metric_result, n_boot, n_sample)
  
  ## this is still messy -- could be cleaner if I fix this before hand?
  # get config and scenario names from the "group_name"
  parts <- unlist(strsplit(group_name, split = "\\."))
  config <- parts[1]
  scenario <- parts[2]
  
  data.frame(
    config = config,
    scenario = scenario,
    spread = spread_vals
  )
})

names(bootstrap_lc_gsat_spread) <- names(lc_gsat_split)

bootstrap_lc_gsat_spread_df <- do.call(rbind, bootstrap_lc_gsat_spread)
```

The result of the above is a new data frame with 10,000 spread estimates for each config-SSP combination. This will allow us to compute summary statistics on the uncertainty spread of our data and also allow us to make comparisons about uncertainty spread across config types.

We can use the bootstrap data to produce statistical summaries fro the uncertainty spread of the different configs across SSP scenarios:
```{r}

spread_summary_df <- bootstrap_lc_gsat_spread_df %>% 
  group_by(config, scenario) %>% 
  summarize(
    mean_spread = mean(spread),
    ci_low = quantile(spread, 0.025), 
    ci_high = quantile(spread, 0.975), 
    .groups = "drop"
  )

```

Before moving on to the next step of looking at pairwise spread differences, we can plot the mean + CI of the spread by config type for each SSP:

```{r}
mean_config_spread <- 
  ggplot(data = spread_summary_df) +
  geom_bar(aes(x = config, y = mean_spread, fill = config), stat = "identity") +
  geom_errorbar(aes(x = config, ymin = ci_low, ymax = ci_high), width = 0.25) +
  facet_wrap(~scenario) +
  theme_light()

print(mean_config_spread)

ggsave("output/figures/mean_config_spread.png", mean_config_spread,
       height = 5, 
       width = 10, 
       units = "in", 
       device = "png")

```

The barplot reveals a consistent difference in the uncertainty spread across model configurations. In nearly every SSP scenario, the concentration-driven (CD) configuration exhibits a significantly narrower spread in late century global temperature projections compared to the emissions-driven (ED) and emissions-with-carbon-cycle (EC) configurations. This suggests that constraining historical CO2 concentrations suppresses carbon cycle variability, which in turn limits the propagation of uncertainty into future projections. This reduced spread could lead to underestimating the likelihood of exceeding key temperature thresholds. This pattern holds across all scenarios tested here, with the exception of SSP5-8.5, where the difference between configurations appears less pronounced.

This analysis also provides useful perspective on a broader modeling practice: Earth system models (ESMs) are typically run in concentration-driven mode for both historical and scenario simulations, meaning CO2 concentrations are prescribed rather than simulated. The SCM experiment here isolates the consequences of that choice. It shows that prescribing CO₂ concentrations—while useful for consistency—can obscure the uncertainty introduced by carbon cycle processes, especially when estimating the likelihood of warming outcomes. This observation reinforces arguments made by Sanderson et al. (2024), who caution that concentration-driven frameworks may under-represent structural uncertainty in future climate projections.

### Pairwise comparisons

Now we want to know the pariwise differences so we can make quantitative statements about the spread differences between the configuration types:
```{r}
# List of SSPs and config pairs
set.seed(444)

ssp_list <- unique(lc_gsat_all$scenario)
config_pairs <- list(
  c("CD", "CE"),
  c("CD", "ED"),
  c("CE", "ED")
)

# Initialize results list
pairwise_results <- list()

# Loop through SSPs and config pairs
for (ssp in ssp_list) {
  for (pair in config_pairs) {
    
    cfg1 <- pair[1]
    cfg2 <- pair[2]
    
    # Build the key names to match the list
    name1 <- paste(cfg1, ssp, sep = ".")
    name2 <- paste(cfg2, ssp, sep = ".")
    
    # Make sure both are in your list
    if (!(name1 %in% names(bootstrap_lc_gsat_spread)) || 
        !(name2 %in% names(bootstrap_lc_gsat_spread))) {
      warning(paste("Missing:", name1, "or", name2))
      next
    }
    
    # Pull spread vectors
    spread1 <- bootstrap_lc_gsat_spread[[name1]]$spread
    spread2 <- bootstrap_lc_gsat_spread[[name2]]$spread
    
    # Subtract
    diff <- spread1 - spread2
    
    # Summarize
    pairwise_results[[paste(ssp, cfg1, cfg2, sep = "_")]] <- data.frame(
      scenario = ssp,
      config_1 = cfg1,
      config_2 = cfg2,
      mean_diff = mean(diff),
      ci_low = quantile(diff, 0.025),
      ci_high = quantile(diff, 0.975)
    )
  }
}

# Combine into final results data frame
pairwise_results_df <- do.call(rbind, pairwise_results)
```

Add a flag to tell which of these is significant:
```{r}

pairwise_results_df$significant <- with(pairwise_results_df, ci_low > 0 | ci_high < 0)
pairwise_results_df$label <- ifelse(pairwise_results_df$significant, "*", "")

```

```{r}
# Define consistent dodge object to reuse across geoms
dodge <- position_dodge(width = 0.6)

pairwise_spread_comparison <- 
  ggplot(pairwise_results_df, 
         aes(x = scenario, y = mean_diff, 
             color = interaction(config_1, config_2),
             group = interaction(config_1, config_2))) +
  geom_point(position = position_dodge(width = 0.6), size = 3) +
  geom_errorbar(aes(ymin = ci_low, ymax = ci_high),
                position = position_dodge(width = 0.6),
                width = 0.25,
                linewidth = 0.8) +
geom_text(aes(label = label, y = ci_high + 0.02),
            position = dodge,
            vjust = 0, size = 6, 
          show.legend = FALSE) +
  geom_hline(yintercept = 0, linetype = "dashed", color = "gray50") +
  scale_color_brewer(palette = "Set2") +
  labs(
    x = "Scenario",
    y = "Difference in spread (C)",
    color = "Comparison",
    title = "Spread differences between configuration pairs (95% CI)"
  ) +
  theme_light(base_size = 12) +
  theme(
    panel.grid.minor = element_blank(),
    axis.text.x = element_text(angle = 30, hjust = 1)
  )

print(pairwise_spread_comparison)

ggsave("output/figures/pairwise_spread_comparison.png", pairwise_spread_comparison,
       height = 5, 
       width = 10, 
       units = "in", 
       device = "png")

```

important interpretation note -- in this type of figure, if the CI line crosses zero, there is a potential that the two configurations do not have a mean uncertainty spread that is different. 

Stab at written results (quantitative values need corrected):
Across all SSP scenarios, the concentration-driven (CD) configuration consistently produced narrower uncertainty spreads than both the emissions-driven (ED) and the concentration-emission (CE) configurations. In every scenario, the mean spread in CD was significantly smaller than in CE and ED. For example, in SSP2-4.5, the spread in CD was on average 0.28 degC narrower than in CE (90% CI: 0.19–0.37 degC), and 0.24 degC narrower than in ED (90% CI: 0.15–0.34 degC). These results support the hypothesis that suppressing carbon cycle uncertainty in the CD configuration artificially (? not sure this is a good use of the word) constrains projected temperature outcomes.

In contrast, differences between CE and ED were small and not statistically significant in any scenario (90% CIs all include zero). This suggests that the act of prescribing CO₂ concentrations, rather than the inclusion or exclusion of the carbon cycle alone, is the primary factor driving the observed reduction in uncertainty.

Does this mean or provide some evidence that our relatively simple sampling procedure from parameters that largely influence the behavior of the carbon cycle naturally lead to physically plausible outcomes? Because our parameter priors were drawn from literature-based distributions consistent with observed carbon cycle behavior, even unweighted emissions-driven simulations produced historical CO₂ concentrations closely aligned with the prescribed values used in CE. This minimized differences in atmospheric CO₂ at the emissions switch point in 2020, reducing divergence between the ED and CE projections.

SIDE NOTE: Maybe also include a comparison where I have emissions-driven with historical CO2 weighting? 